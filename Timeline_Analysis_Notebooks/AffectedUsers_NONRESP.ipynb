{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports and includes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import matplotlib.pyplot as plt \n",
    "import pickle\n",
    "import statistics\n",
    "import numpy as np  \n",
    "import sys\n",
    "import glob\n",
    "\n",
    "pd.set_option('display.max_rows', 4000)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment analysis function provided by Upasana**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_score_for_englishOnlyTweet(cleaned_tweet): #this function takes in a tweet that has been detected as an english tweet\n",
    "    score = analyser.polarity_scores(cleaned_tweet) #so this cleaned_tweet is always an english tweet.\n",
    "    lb = score['compound']\n",
    "    return lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create list of all Baseline Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_lists=[\n",
    "    \"/home/updu6059/Twint_Data/ParallelProcessing/Famous_Users/DidNotMentionBack_Users/Filtered_EnglishUsers_DidNotMentionedBackUsers_FINAL.csv\",\n",
    "# #     \"/home/updu6059/Twint_Data/ParallelProcessing/Famous_Users/MentionedBack_Users/Filtered_EnglishUsers_MentionedBackUsers_FINAL.csv\",\n",
    "    '../BeforeAndAfterAnalysis/Filtered_EnglishUsers_DidnotMentionBackUsers.csv']\n",
    "#     '../BeforeAndAfterAnalysis/Filtered_EnglishUsers_MentionBackedUsers.csv']\n",
    "\n",
    "UserList = []\n",
    "userPathDict = {}\n",
    "for i in range(2):  \n",
    "    df = pd.read_csv(all_user_lists[i])\n",
    "    tempUserList = list(df['Users'])\n",
    "    for j in range(len(tempUserList)):\n",
    "        UserList.append(tempUserList[j])\n",
    "        if (i == 0) : userPathDict[tempUserList[j]] = '/home/updu6059/Twint_Data/ParallelProcessing/Famous_Users/User_Data/English_Only_Tweets/'\n",
    "#         elif (i == 1) : userPathDict[tempUserList[j]] = '/home/updu6059/Twint_Data/ParallelProcessing/Famous_Users/User_Data/English_Only_Tweets/'\n",
    "        elif (i == 1) : userPathDict[tempUserList[j]] = '/home/updu6059/Twint_Data/ParallelProcessing/OnlyEnglishTweets/'\n",
    "#         elif (i == 3) : userPathDict[tempUserList[j]] = '/home/updu6059/Twint_Data/ParallelProcessing/OnlyEnglishTweets/'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possibly affected users: 4368\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of possibly affected users: \" + str(len(UserList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pau_all.txt', 'w') as filehandle:\n",
    "#     for listitem in UserList:\n",
    "#         filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to check for a mention of Donald Trump or Hillary Clinton in a tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mentions_trump(mention_string):\n",
    "    if mention_string != \"[]\": \n",
    "        mention_list = mention_string.split(', ')\n",
    "        mention_list[0] = mention_list[0][1:]\n",
    "        mention_list[-1] = mention_list[-1][:-1]\n",
    "        for x in range(len(mention_list)):\n",
    "            mention_list[x] = mention_list[x][1:]\n",
    "            mention_list[x] = mention_list[x][:-1]\n",
    "        for x in range(len(mention_list)):\n",
    "            if mention_list[x] == 'realdonaldtrump':\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mentions_clinton(mention_string):\n",
    "    if mention_string != \"[]\": \n",
    "        mention_list = mention_string.split(', ')\n",
    "        mention_list[0] = mention_list[0][1:]\n",
    "        mention_list[-1] = mention_list[-1][:-1]\n",
    "        for x in range(len(mention_list)):\n",
    "            mention_list[x] = mention_list[x][1:]\n",
    "            mention_list[x] = mention_list[x][:-1]\n",
    "        for x in range(len(mention_list)):\n",
    "            if mention_list[x] == 'hillaryclinton':\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate monthy values for each user (over the 96 months from Jan 2009 to December 2016)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "timelineDict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstTweetIndexDict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "timelineDF = pd.DataFrame(columns=['username','tweet_count','sentiment_scores', \n",
    "                                      'trump_count', 'clinton_count','trump_sent','clinton_sent',\n",
    "                                      'positive_sent', 'negative_sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4368\n",
      "1 / 4368\n",
      "2 / 4368\n",
      "3 / 4368\n",
      "4 / 4368\n",
      "5 / 4368\n",
      "6 / 4368\n",
      "7 / 4368\n",
      "8 / 4368\n",
      "9 / 4368\n",
      "10 / 4368\n",
      "11 / 4368\n",
      "12 / 4368\n",
      "13 / 4368\n",
      "14 / 4368\n",
      "15 / 4368\n",
      "16 / 4368\n",
      "17 / 4368\n",
      "18 / 4368\n",
      "19 / 4368\n",
      "20 / 4368\n",
      "21 / 4368\n",
      "22 / 4368\n",
      "23 / 4368\n",
      "24 / 4368\n",
      "25 / 4368\n",
      "26 / 4368\n",
      "27 / 4368\n",
      "28 / 4368\n",
      "29 / 4368\n",
      "30 / 4368\n",
      "31 / 4368\n",
      "32 / 4368\n",
      "33 / 4368\n",
      "34 / 4368\n",
      "35 / 4368\n",
      "36 / 4368\n",
      "37 / 4368\n",
      "38 / 4368\n",
      "39 / 4368\n",
      "40 / 4368\n",
      "41 / 4368\n",
      "42 / 4368\n",
      "43 / 4368\n",
      "44 / 4368\n",
      "45 / 4368\n",
      "46 / 4368\n",
      "47 / 4368\n",
      "48 / 4368\n",
      "49 / 4368\n",
      "50 / 4368\n",
      "51 / 4368\n",
      "52 / 4368\n",
      "53 / 4368\n",
      "54 / 4368\n",
      "55 / 4368\n",
      "56 / 4368\n",
      "57 / 4368\n",
      "58 / 4368\n",
      "59 / 4368\n",
      "60 / 4368\n",
      "61 / 4368\n",
      "62 / 4368\n",
      "63 / 4368\n",
      "64 / 4368\n",
      "65 / 4368\n",
      "66 / 4368\n",
      "67 / 4368\n",
      "68 / 4368\n",
      "69 / 4368\n",
      "70 / 4368\n",
      "71 / 4368\n",
      "72 / 4368\n",
      "73 / 4368\n",
      "74 / 4368\n",
      "75 / 4368\n",
      "76 / 4368\n",
      "77 / 4368\n",
      "78 / 4368\n",
      "79 / 4368\n",
      "80 / 4368\n",
      "81 / 4368\n",
      "82 / 4368\n",
      "83 / 4368\n",
      "84 / 4368\n",
      "85 / 4368\n",
      "86 / 4368\n",
      "87 / 4368\n",
      "88 / 4368\n",
      "89 / 4368\n",
      "90 / 4368\n",
      "91 / 4368\n",
      "92 / 4368\n",
      "93 / 4368\n",
      "94 / 4368\n",
      "95 / 4368\n",
      "96 / 4368\n",
      "97 / 4368\n",
      "98 / 4368\n",
      "99 / 4368\n",
      "100 / 4368\n",
      "101 / 4368\n",
      "102 / 4368\n",
      "103 / 4368\n",
      "104 / 4368\n",
      "105 / 4368\n",
      "106 / 4368\n",
      "107 / 4368\n",
      "108 / 4368\n",
      "109 / 4368\n",
      "110 / 4368\n",
      "111 / 4368\n",
      "112 / 4368\n",
      "113 / 4368\n",
      "114 / 4368\n",
      "115 / 4368\n",
      "116 / 4368\n",
      "117 / 4368\n",
      "118 / 4368\n",
      "119 / 4368\n",
      "120 / 4368\n",
      "121 / 4368\n",
      "122 / 4368\n",
      "123 / 4368\n",
      "124 / 4368\n",
      "125 / 4368\n",
      "126 / 4368\n",
      "127 / 4368\n",
      "128 / 4368\n",
      "129 / 4368\n",
      "130 / 4368\n",
      "131 / 4368\n",
      "132 / 4368\n",
      "133 / 4368\n",
      "134 / 4368\n",
      "135 / 4368\n",
      "136 / 4368\n",
      "137 / 4368\n",
      "138 / 4368\n",
      "139 / 4368\n",
      "140 / 4368\n",
      "141 / 4368\n",
      "142 / 4368\n",
      "143 / 4368\n",
      "144 / 4368\n",
      "145 / 4368\n",
      "146 / 4368\n",
      "147 / 4368\n",
      "148 / 4368\n",
      "149 / 4368\n",
      "150 / 4368\n",
      "151 / 4368\n",
      "152 / 4368\n",
      "153 / 4368\n",
      "154 / 4368\n",
      "155 / 4368\n",
      "156 / 4368\n",
      "157 / 4368\n",
      "158 / 4368\n",
      "159 / 4368\n",
      "160 / 4368\n",
      "161 / 4368\n",
      "162 / 4368\n",
      "163 / 4368\n",
      "164 / 4368\n",
      "165 / 4368\n",
      "166 / 4368\n",
      "167 / 4368\n",
      "168 / 4368\n",
      "169 / 4368\n",
      "170 / 4368\n",
      "171 / 4368\n",
      "172 / 4368\n",
      "173 / 4368\n",
      "174 / 4368\n",
      "175 / 4368\n",
      "176 / 4368\n",
      "177 / 4368\n",
      "178 / 4368\n",
      "179 / 4368\n",
      "180 / 4368\n",
      "181 / 4368\n",
      "182 / 4368\n",
      "183 / 4368\n",
      "184 / 4368\n",
      "185 / 4368\n",
      "186 / 4368\n",
      "187 / 4368\n",
      "188 / 4368\n",
      "189 / 4368\n",
      "190 / 4368\n",
      "191 / 4368\n",
      "192 / 4368\n",
      "193 / 4368\n",
      "194 / 4368\n",
      "195 / 4368\n",
      "196 / 4368\n",
      "197 / 4368\n",
      "198 / 4368\n",
      "199 / 4368\n",
      "200 / 4368\n",
      "201 / 4368\n",
      "202 / 4368\n",
      "203 / 4368\n",
      "204 / 4368\n",
      "205 / 4368\n",
      "206 / 4368\n",
      "207 / 4368\n",
      "208 / 4368\n",
      "209 / 4368\n",
      "210 / 4368\n",
      "211 / 4368\n",
      "212 / 4368\n",
      "213 / 4368\n",
      "214 / 4368\n",
      "215 / 4368\n",
      "216 / 4368\n",
      "217 / 4368\n",
      "218 / 4368\n",
      "219 / 4368\n",
      "220 / 4368\n",
      "221 / 4368\n",
      "222 / 4368\n",
      "223 / 4368\n",
      "224 / 4368\n",
      "225 / 4368\n",
      "226 / 4368\n",
      "227 / 4368\n",
      "228 / 4368\n",
      "229 / 4368\n",
      "230 / 4368\n",
      "231 / 4368\n",
      "232 / 4368\n",
      "233 / 4368\n",
      "234 / 4368\n",
      "235 / 4368\n",
      "236 / 4368\n",
      "237 / 4368\n",
      "238 / 4368\n",
      "239 / 4368\n",
      "240 / 4368\n",
      "241 / 4368\n",
      "242 / 4368\n",
      "243 / 4368\n",
      "244 / 4368\n",
      "245 / 4368\n",
      "246 / 4368\n",
      "247 / 4368\n",
      "248 / 4368\n",
      "249 / 4368\n",
      "250 / 4368\n",
      "251 / 4368\n",
      "252 / 4368\n",
      "253 / 4368\n",
      "254 / 4368\n",
      "255 / 4368\n",
      "256 / 4368\n",
      "257 / 4368\n",
      "258 / 4368\n",
      "259 / 4368\n",
      "260 / 4368\n",
      "261 / 4368\n",
      "262 / 4368\n",
      "263 / 4368\n",
      "264 / 4368\n",
      "265 / 4368\n",
      "266 / 4368\n",
      "267 / 4368\n",
      "268 / 4368\n",
      "269 / 4368\n",
      "270 / 4368\n",
      "271 / 4368\n",
      "272 / 4368\n",
      "273 / 4368\n",
      "274 / 4368\n",
      "275 / 4368\n",
      "276 / 4368\n",
      "277 / 4368\n",
      "278 / 4368\n",
      "279 / 4368\n",
      "280 / 4368\n",
      "281 / 4368\n",
      "282 / 4368\n",
      "283 / 4368\n",
      "284 / 4368\n",
      "285 / 4368\n",
      "286 / 4368\n",
      "287 / 4368\n",
      "288 / 4368\n",
      "289 / 4368\n",
      "290 / 4368\n",
      "291 / 4368\n",
      "292 / 4368\n",
      "293 / 4368\n",
      "294 / 4368\n",
      "295 / 4368\n",
      "296 / 4368\n",
      "297 / 4368\n",
      "298 / 4368\n",
      "299 / 4368\n",
      "300 / 4368\n",
      "301 / 4368\n",
      "302 / 4368\n",
      "303 / 4368\n",
      "304 / 4368\n",
      "305 / 4368\n",
      "306 / 4368\n",
      "307 / 4368\n",
      "308 / 4368\n",
      "309 / 4368\n",
      "310 / 4368\n",
      "311 / 4368\n",
      "312 / 4368\n",
      "313 / 4368\n",
      "314 / 4368\n",
      "315 / 4368\n",
      "316 / 4368\n",
      "317 / 4368\n",
      "318 / 4368\n",
      "319 / 4368\n",
      "320 / 4368\n",
      "321 / 4368\n",
      "322 / 4368\n",
      "323 / 4368\n",
      "324 / 4368\n",
      "325 / 4368\n",
      "326 / 4368\n",
      "327 / 4368\n",
      "328 / 4368\n",
      "329 / 4368\n",
      "330 / 4368\n",
      "331 / 4368\n",
      "332 / 4368\n",
      "333 / 4368\n",
      "334 / 4368\n",
      "335 / 4368\n",
      "336 / 4368\n",
      "337 / 4368\n",
      "338 / 4368\n",
      "339 / 4368\n",
      "340 / 4368\n",
      "341 / 4368\n",
      "342 / 4368\n",
      "343 / 4368\n",
      "344 / 4368\n",
      "345 / 4368\n",
      "346 / 4368\n",
      "347 / 4368\n",
      "348 / 4368\n",
      "349 / 4368\n",
      "350 / 4368\n",
      "351 / 4368\n",
      "352 / 4368\n",
      "353 / 4368\n",
      "354 / 4368\n",
      "355 / 4368\n",
      "356 / 4368\n",
      "357 / 4368\n",
      "358 / 4368\n",
      "359 / 4368\n",
      "360 / 4368\n",
      "361 / 4368\n",
      "362 / 4368\n",
      "363 / 4368\n",
      "364 / 4368\n",
      "365 / 4368\n",
      "366 / 4368\n",
      "367 / 4368\n",
      "368 / 4368\n",
      "369 / 4368\n",
      "370 / 4368\n",
      "371 / 4368\n",
      "372 / 4368\n",
      "373 / 4368\n",
      "374 / 4368\n",
      "375 / 4368\n",
      "376 / 4368\n",
      "377 / 4368\n",
      "378 / 4368\n",
      "379 / 4368\n",
      "380 / 4368\n",
      "381 / 4368\n",
      "382 / 4368\n",
      "383 / 4368\n",
      "384 / 4368\n",
      "385 / 4368\n",
      "386 / 4368\n",
      "387 / 4368\n",
      "388 / 4368\n",
      "389 / 4368\n",
      "390 / 4368\n",
      "391 / 4368\n",
      "392 / 4368\n",
      "393 / 4368\n",
      "394 / 4368\n",
      "395 / 4368\n",
      "396 / 4368\n",
      "397 / 4368\n",
      "398 / 4368\n",
      "399 / 4368\n",
      "400 / 4368\n",
      "401 / 4368\n",
      "402 / 4368\n",
      "403 / 4368\n",
      "404 / 4368\n",
      "405 / 4368\n",
      "406 / 4368\n",
      "407 / 4368\n",
      "408 / 4368\n",
      "409 / 4368\n",
      "410 / 4368\n",
      "411 / 4368\n",
      "412 / 4368\n",
      "413 / 4368\n",
      "414 / 4368\n",
      "415 / 4368\n",
      "416 / 4368\n",
      "417 / 4368\n",
      "418 / 4368\n",
      "419 / 4368\n",
      "420 / 4368\n",
      "421 / 4368\n",
      "422 / 4368\n",
      "423 / 4368\n",
      "424 / 4368\n",
      "425 / 4368\n",
      "426 / 4368\n",
      "427 / 4368\n",
      "428 / 4368\n",
      "429 / 4368\n",
      "430 / 4368\n",
      "431 / 4368\n",
      "432 / 4368\n",
      "433 / 4368\n",
      "434 / 4368\n",
      "435 / 4368\n",
      "436 / 4368\n",
      "437 / 4368\n",
      "438 / 4368\n",
      "439 / 4368\n",
      "440 / 4368\n",
      "441 / 4368\n",
      "442 / 4368\n",
      "443 / 4368\n",
      "444 / 4368\n",
      "445 / 4368\n",
      "446 / 4368\n",
      "447 / 4368\n",
      "448 / 4368\n",
      "449 / 4368\n",
      "450 / 4368\n",
      "451 / 4368\n",
      "452 / 4368\n",
      "453 / 4368\n",
      "454 / 4368\n",
      "455 / 4368\n",
      "456 / 4368\n",
      "457 / 4368\n",
      "458 / 4368\n",
      "459 / 4368\n",
      "460 / 4368\n",
      "461 / 4368\n",
      "462 / 4368\n",
      "463 / 4368\n",
      "464 / 4368\n",
      "465 / 4368\n",
      "466 / 4368\n",
      "467 / 4368\n",
      "468 / 4368\n",
      "469 / 4368\n",
      "470 / 4368\n",
      "471 / 4368\n",
      "472 / 4368\n",
      "473 / 4368\n",
      "474 / 4368\n",
      "475 / 4368\n",
      "476 / 4368\n",
      "477 / 4368\n",
      "478 / 4368\n",
      "479 / 4368\n",
      "480 / 4368\n",
      "481 / 4368\n",
      "482 / 4368\n",
      "483 / 4368\n",
      "484 / 4368\n",
      "485 / 4368\n",
      "486 / 4368\n",
      "487 / 4368\n",
      "488 / 4368\n",
      "489 / 4368\n",
      "490 / 4368\n",
      "491 / 4368\n",
      "492 / 4368\n",
      "493 / 4368\n",
      "494 / 4368\n",
      "495 / 4368\n",
      "496 / 4368\n",
      "497 / 4368\n",
      "498 / 4368\n",
      "499 / 4368\n",
      "500 / 4368\n",
      "501 / 4368\n",
      "502 / 4368\n",
      "503 / 4368\n",
      "504 / 4368\n",
      "505 / 4368\n",
      "506 / 4368\n",
      "507 / 4368\n",
      "508 / 4368\n",
      "509 / 4368\n",
      "510 / 4368\n",
      "511 / 4368\n",
      "512 / 4368\n",
      "513 / 4368\n",
      "514 / 4368\n",
      "515 / 4368\n",
      "516 / 4368\n",
      "517 / 4368\n",
      "518 / 4368\n",
      "519 / 4368\n",
      "520 / 4368\n",
      "521 / 4368\n",
      "522 / 4368\n",
      "523 / 4368\n",
      "524 / 4368\n",
      "525 / 4368\n",
      "526 / 4368\n",
      "527 / 4368\n",
      "528 / 4368\n",
      "529 / 4368\n",
      "530 / 4368\n",
      "531 / 4368\n",
      "532 / 4368\n",
      "533 / 4368\n",
      "534 / 4368\n",
      "535 / 4368\n",
      "536 / 4368\n",
      "537 / 4368\n",
      "538 / 4368\n",
      "539 / 4368\n",
      "540 / 4368\n",
      "541 / 4368\n",
      "542 / 4368\n",
      "543 / 4368\n",
      "544 / 4368\n",
      "545 / 4368\n",
      "546 / 4368\n",
      "547 / 4368\n",
      "548 / 4368\n",
      "549 / 4368\n",
      "550 / 4368\n",
      "551 / 4368\n",
      "552 / 4368\n",
      "553 / 4368\n",
      "554 / 4368\n",
      "555 / 4368\n",
      "556 / 4368\n",
      "557 / 4368\n",
      "558 / 4368\n",
      "559 / 4368\n",
      "560 / 4368\n",
      "561 / 4368\n",
      "562 / 4368\n",
      "563 / 4368\n",
      "564 / 4368\n",
      "565 / 4368\n",
      "566 / 4368\n",
      "567 / 4368\n",
      "568 / 4368\n",
      "569 / 4368\n",
      "570 / 4368\n",
      "571 / 4368\n",
      "572 / 4368\n",
      "573 / 4368\n",
      "574 / 4368\n",
      "575 / 4368\n",
      "576 / 4368\n",
      "577 / 4368\n",
      "578 / 4368\n",
      "579 / 4368\n",
      "580 / 4368\n",
      "581 / 4368\n",
      "582 / 4368\n",
      "583 / 4368\n",
      "584 / 4368\n",
      "585 / 4368\n",
      "586 / 4368\n",
      "587 / 4368\n",
      "588 / 4368\n",
      "589 / 4368\n",
      "590 / 4368\n",
      "591 / 4368\n",
      "592 / 4368\n",
      "593 / 4368\n",
      "594 / 4368\n",
      "595 / 4368\n",
      "596 / 4368\n",
      "597 / 4368\n",
      "598 / 4368\n",
      "599 / 4368\n",
      "600 / 4368\n",
      "601 / 4368\n",
      "602 / 4368\n",
      "603 / 4368\n",
      "604 / 4368\n",
      "605 / 4368\n",
      "606 / 4368\n",
      "607 / 4368\n",
      "608 / 4368\n",
      "609 / 4368\n",
      "610 / 4368\n",
      "611 / 4368\n",
      "612 / 4368\n",
      "613 / 4368\n",
      "614 / 4368\n",
      "615 / 4368\n",
      "616 / 4368\n",
      "617 / 4368\n",
      "618 / 4368\n",
      "619 / 4368\n",
      "620 / 4368\n",
      "621 / 4368\n",
      "622 / 4368\n",
      "623 / 4368\n",
      "624 / 4368\n",
      "625 / 4368\n",
      "626 / 4368\n",
      "627 / 4368\n",
      "628 / 4368\n",
      "629 / 4368\n",
      "630 / 4368\n",
      "631 / 4368\n",
      "632 / 4368\n",
      "633 / 4368\n",
      "634 / 4368\n",
      "635 / 4368\n",
      "636 / 4368\n",
      "637 / 4368\n",
      "638 / 4368\n",
      "639 / 4368\n",
      "640 / 4368\n",
      "641 / 4368\n",
      "642 / 4368\n",
      "643 / 4368\n",
      "644 / 4368\n",
      "645 / 4368\n",
      "646 / 4368\n",
      "647 / 4368\n",
      "648 / 4368\n",
      "649 / 4368\n",
      "650 / 4368\n",
      "651 / 4368\n",
      "652 / 4368\n",
      "653 / 4368\n",
      "654 / 4368\n",
      "655 / 4368\n",
      "656 / 4368\n",
      "657 / 4368\n",
      "658 / 4368\n",
      "659 / 4368\n",
      "660 / 4368\n",
      "661 / 4368\n",
      "662 / 4368\n",
      "663 / 4368\n",
      "664 / 4368\n",
      "665 / 4368\n",
      "666 / 4368\n",
      "667 / 4368\n",
      "668 / 4368\n",
      "669 / 4368\n",
      "670 / 4368\n",
      "671 / 4368\n",
      "672 / 4368\n",
      "673 / 4368\n",
      "674 / 4368\n",
      "675 / 4368\n",
      "676 / 4368\n",
      "677 / 4368\n",
      "678 / 4368\n",
      "679 / 4368\n",
      "680 / 4368\n",
      "681 / 4368\n",
      "682 / 4368\n",
      "683 / 4368\n",
      "684 / 4368\n",
      "685 / 4368\n",
      "686 / 4368\n",
      "687 / 4368\n",
      "688 / 4368\n",
      "689 / 4368\n",
      "690 / 4368\n",
      "691 / 4368\n",
      "692 / 4368\n",
      "693 / 4368\n",
      "694 / 4368\n",
      "695 / 4368\n",
      "696 / 4368\n",
      "697 / 4368\n",
      "698 / 4368\n",
      "699 / 4368\n",
      "700 / 4368\n",
      "701 / 4368\n",
      "702 / 4368\n",
      "703 / 4368\n",
      "704 / 4368\n",
      "705 / 4368\n",
      "706 / 4368\n",
      "707 / 4368\n",
      "708 / 4368\n",
      "709 / 4368\n",
      "710 / 4368\n",
      "711 / 4368\n",
      "712 / 4368\n",
      "713 / 4368\n",
      "714 / 4368\n",
      "715 / 4368\n",
      "716 / 4368\n",
      "717 / 4368\n",
      "718 / 4368\n",
      "719 / 4368\n",
      "720 / 4368\n",
      "721 / 4368\n",
      "722 / 4368\n",
      "723 / 4368\n",
      "724 / 4368\n",
      "725 / 4368\n",
      "726 / 4368\n",
      "727 / 4368\n",
      "728 / 4368\n",
      "729 / 4368\n",
      "730 / 4368\n",
      "731 / 4368\n",
      "732 / 4368\n",
      "733 / 4368\n",
      "734 / 4368\n",
      "735 / 4368\n",
      "736 / 4368\n",
      "737 / 4368\n",
      "738 / 4368\n",
      "739 / 4368\n",
      "740 / 4368\n",
      "741 / 4368\n",
      "742 / 4368\n",
      "743 / 4368\n",
      "744 / 4368\n",
      "745 / 4368\n",
      "746 / 4368\n",
      "747 / 4368\n",
      "748 / 4368\n",
      "749 / 4368\n",
      "750 / 4368\n"
     ]
    }
   ],
   "source": [
    "size = str(len(UserList))\n",
    "for i in range(len(UserList)):\n",
    "# for i in range(100):\n",
    "    try:\n",
    "        #read in, reverse order (so oldest is first), reset index\n",
    "        temp_user_db = pd.read_csv(userPathDict[UserList[i]]+UserList[i]+'.csv', header=None, low_memory=False)\n",
    "        temp_user_db = temp_user_db.iloc[::-1]\n",
    "        temp_user_db.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #create lists for each metric\n",
    "        tweet_count = [\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        trump_count = [\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        clinton_count = [\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        sentiment_scores = [\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "        trump_sent = [\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "        clinton_sent = [\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "        positive_sent = [\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "        negative_sent = [\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[],\n",
    "            [],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "\n",
    "        #iterate through user tweets\n",
    "        first = 1\n",
    "        for j in range(len(temp_user_db)):\n",
    "\n",
    "            #step one is assigning an index for the tweet, based on date, or disregarding\n",
    "            found = 0\n",
    "            tweetdate = dt.strptime(temp_user_db[3][j], \"%Y-%m-%d\")\n",
    "            for k in range(2009,2017):\n",
    "                temp_year = k\n",
    "                date_start = dt(temp_year,1,1)\n",
    "                date_end = dt(temp_year,12,31)\n",
    "                if tweetdate < date_start:\n",
    "                    continue;\n",
    "                elif tweetdate <= date_end:\n",
    "                    for m in range(2,14):\n",
    "                        if m == 13: #month must be December by process of elimination\n",
    "                            use_index = (((k-2009)*12)+(m-2))\n",
    "                            found = 1\n",
    "    #                         print(temp_user_db['date'][j] + \" \" +str(use_index))\n",
    "                            break;\n",
    "                        next_month_start = dt(temp_year,m,1)\n",
    "                        if tweetdate < next_month_start:\n",
    "                            use_index = (((k-2009)*12)+(m-2))\n",
    "                            found = 1\n",
    "    #                         print(temp_user_db['date'][j] + \" \" +str(use_index))\n",
    "                            break;\n",
    "\n",
    "            #tweet occurred within the period 2009-2016 and was assigned an valid index\n",
    "            if found == 1:\n",
    "                if first:\n",
    "                    firstTweetIndexDict[UserList[i]] = use_index\n",
    "                    first = 0\n",
    "\n",
    "                tweet_count[use_index] +=1\n",
    "\n",
    "                tweet_sent = sentiment_analyzer_score_for_englishOnlyTweet(temp_user_db[10][j])\n",
    "                sentiment_scores[use_index].append(tweet_sent)\n",
    "\n",
    "                trump_flag = mentions_trump(str(temp_user_db[11][j]))\n",
    "                clinton_flag = mentions_clinton(str(temp_user_db[11][j]))\n",
    "\n",
    "                if trump_flag: trump_count[use_index] += 1\n",
    "                if clinton_flag: clinton_count[use_index] += 1\n",
    "\n",
    "                if trump_flag and not clinton_flag: trump_sent[use_index].append(tweet_sent)\n",
    "                if clinton_flag and not trump_flag: clinton_sent[use_index].append(tweet_sent)\n",
    "\n",
    "                if tweet_sent > 0: positive_sent[use_index].append(tweet_sent)\n",
    "                if tweet_sent < 0: negative_sent[use_index].append(tweet_sent)\n",
    "\n",
    "        timelineDict[UserList[i]] = [tweet_count, sentiment_scores, trump_count, clinton_count, \n",
    "                                     trump_sent, clinton_sent, positive_sent, negative_sent]\n",
    "\n",
    "        print(str(i) + \" / \" + size)\n",
    "\n",
    "    except Exception as e: \n",
    "        print(str(UserList[i]) + \" failed - \" + str(e))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(firstTweetIndexDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(timelineDF)):\n",
    "    try:\n",
    "        test = firstTweetIndexDict[timelineDF['username'][i]]\n",
    "    except:\n",
    "        print(timelineDF['username'][i] + ' failed [' +str(i)+']' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timelineDF = pd.DataFrame.from_dict(timelineDict, orient='index',columns=['tweet_count','sentiment_scores', \n",
    "                                      'trump_count', 'clinton_count','trump_sent','clinton_sent',\n",
    "                                      'positive_sent', 'negative_sent'])\n",
    "\n",
    "timelineDF.index.name = 'username'\n",
    "timelineDF.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(timelineDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_countDict = {}\n",
    "sentiment_scoresDict = {}\n",
    "trump_countDict = {}\n",
    "clinton_countDict = {}\n",
    "trump_sentDict = {}\n",
    "clinton_sentDict = {}\n",
    "positive_sentDict = {}\n",
    "negative_sentDict = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size = str(len(timelineDF))\n",
    "for i in range(len(timelineDF)):\n",
    "    try:\n",
    "        start_index = firstTweetIndexDict[timelineDF['username'][i]]\n",
    "\n",
    "        tweet_count = []\n",
    "        sentiment_scores = []\n",
    "        trump_count = []\n",
    "        clinton_count = []\n",
    "        trump_sent = []\n",
    "        clinton_sent = []\n",
    "        positive_sent = []\n",
    "        negative_sent = [] \n",
    "\n",
    "        for j in range(0,96):\n",
    "            if j >= start_index:\n",
    "                tweet_count.append(timelineDF['tweet_count'][i][j])\n",
    "                trump_count.append(timelineDF['trump_count'][i][j])\n",
    "                clinton_count.append(timelineDF['clinton_count'][i][j])\n",
    "\n",
    "                if(len(timelineDF['sentiment_scores'][i][j]) > 0):\n",
    "                    sentiment_scores.append(statistics.mean(timelineDF['sentiment_scores'][i][j]))\n",
    "                else:\n",
    "                    sentiment_scores.append(np.nan)\n",
    "\n",
    "                if(len(timelineDF['trump_sent'][i][j]) > 0):\n",
    "                    trump_sent.append(statistics.mean(timelineDF['trump_sent'][i][j]))\n",
    "                else:\n",
    "                    trump_sent.append(np.nan)\n",
    "\n",
    "                if(len(timelineDF['clinton_sent'][i][j]) > 0):\n",
    "                    clinton_sent.append(statistics.mean(timelineDF['clinton_sent'][i][j]))\n",
    "                else:\n",
    "                    clinton_sent.append(np.nan)\n",
    "\n",
    "                if(len(timelineDF['positive_sent'][i][j]) > 0):\n",
    "                    positive_sent.append(statistics.mean(timelineDF['positive_sent'][i][j]))\n",
    "                else:\n",
    "                    positive_sent.append(np.nan)\n",
    "\n",
    "                if(len(timelineDF['negative_sent'][i][j]) > 0):\n",
    "                    negative_sent.append(statistics.mean(timelineDF['negative_sent'][i][j]))\n",
    "                else:\n",
    "                    negative_sent.append(np.nan)\n",
    "\n",
    "            else:\n",
    "                tweet_count.append(np.nan)\n",
    "                trump_count.append(np.nan)\n",
    "                clinton_count.append(np.nan)\n",
    "                sentiment_scores.append(np.nan)\n",
    "                trump_sent.append(np.nan)\n",
    "                clinton_sent.append(np.nan)\n",
    "                positive_sent.append(np.nan)\n",
    "                negative_sent.append(np.nan)\n",
    "\n",
    "        tweet_countDict[timelineDF['username'][i]] = tweet_count\n",
    "        sentiment_scoresDict[timelineDF['username'][i]] = sentiment_scores\n",
    "        trump_countDict[timelineDF['username'][i]] = trump_count\n",
    "        clinton_countDict[timelineDF['username'][i]] = clinton_count\n",
    "        trump_sentDict[timelineDF['username'][i]] = trump_sent\n",
    "        clinton_sentDict[timelineDF['username'][i]] = clinton_sent\n",
    "        positive_sentDict[timelineDF['username'][i]] = positive_sent\n",
    "        negative_sentDict[timelineDF['username'][i]] =  negative_sent \n",
    "\n",
    "        print(str(i) + \" / \" + size)\n",
    "        \n",
    "    except Exception as e: \n",
    "        print(str(timelineDF['username'][i]) + \" failed - \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_countDF = pd.DataFrame.from_dict(tweet_countDict, orient='index',columns=['01-2009','02-2009','03-2009','04-2009','05-2009','06-2009','07-2009','08-2009','09-2009','10-2009','11-2009','12-2009',\n",
    "'01-2010','02-2010','03-2010','04-2010','05-2010','06-2010','07-2010','08-2010','09-2010','10-2010','11-2010','12-2010',\n",
    "'01-2011','02-2011','03-2011','04-2011','05-2011','06-2011','07-2011','08-2011','09-2011','10-2011','11-2011','12-2011',\n",
    "'01-2012','02-2012','03-2012','04-2012','05-2012','06-2012','07-2012','08-2012','09-2012','10-2012','11-2012','12-2012',\n",
    "'01-2013','02-2013','03-2013','04-2013','05-2013','06-2013','07-2013','08-2013','09-2013','10-2013','11-2013','12-2013',\n",
    "'01-2014','02-2014','03-2014','04-2014','05-2014','06-2014','07-2014','08-2014','09-2014','10-2014','11-2014','12-2014',\n",
    "'01-2015','02-2015','03-2015','04-2015','05-2015','06-2015','07-2015','08-2015','09-2015','10-2015','11-2015','12-2015',\n",
    "'01-2016','02-2016','03-2016','04-2016','05-2016','06-2016','07-2016','08-2016','09-2016','10-2016','11-2016','12-2016'])\n",
    "tweet_countDF.index.name = 'username'\n",
    "tweet_countDF.reset_index(inplace=True)\n",
    "tweet_countDF = tweet_countDF.drop(columns=['username'])\n",
    "\n",
    "trump_countDF = pd.DataFrame.from_dict(trump_countDict, orient='index',columns=['01-2009','02-2009','03-2009','04-2009','05-2009','06-2009','07-2009','08-2009','09-2009','10-2009','11-2009','12-2009',\n",
    "'01-2010','02-2010','03-2010','04-2010','05-2010','06-2010','07-2010','08-2010','09-2010','10-2010','11-2010','12-2010',\n",
    "'01-2011','02-2011','03-2011','04-2011','05-2011','06-2011','07-2011','08-2011','09-2011','10-2011','11-2011','12-2011',\n",
    "'01-2012','02-2012','03-2012','04-2012','05-2012','06-2012','07-2012','08-2012','09-2012','10-2012','11-2012','12-2012',\n",
    "'01-2013','02-2013','03-2013','04-2013','05-2013','06-2013','07-2013','08-2013','09-2013','10-2013','11-2013','12-2013',\n",
    "'01-2014','02-2014','03-2014','04-2014','05-2014','06-2014','07-2014','08-2014','09-2014','10-2014','11-2014','12-2014',\n",
    "'01-2015','02-2015','03-2015','04-2015','05-2015','06-2015','07-2015','08-2015','09-2015','10-2015','11-2015','12-2015',\n",
    "'01-2016','02-2016','03-2016','04-2016','05-2016','06-2016','07-2016','08-2016','09-2016','10-2016','11-2016','12-2016'])\n",
    "trump_countDF.index.name = 'username'\n",
    "trump_countDF.reset_index(inplace=True)\n",
    "trump_countDF = trump_countDF.drop(columns=['username'])\n",
    "\n",
    "clinton_countDF = pd.DataFrame.from_dict(clinton_countDict, orient='index',columns=['01-2009','02-2009','03-2009','04-2009','05-2009','06-2009','07-2009','08-2009','09-2009','10-2009','11-2009','12-2009',\n",
    "'01-2010','02-2010','03-2010','04-2010','05-2010','06-2010','07-2010','08-2010','09-2010','10-2010','11-2010','12-2010',\n",
    "'01-2011','02-2011','03-2011','04-2011','05-2011','06-2011','07-2011','08-2011','09-2011','10-2011','11-2011','12-2011',\n",
    "'01-2012','02-2012','03-2012','04-2012','05-2012','06-2012','07-2012','08-2012','09-2012','10-2012','11-2012','12-2012',\n",
    "'01-2013','02-2013','03-2013','04-2013','05-2013','06-2013','07-2013','08-2013','09-2013','10-2013','11-2013','12-2013',\n",
    "'01-2014','02-2014','03-2014','04-2014','05-2014','06-2014','07-2014','08-2014','09-2014','10-2014','11-2014','12-2014',\n",
    "'01-2015','02-2015','03-2015','04-2015','05-2015','06-2015','07-2015','08-2015','09-2015','10-2015','11-2015','12-2015',\n",
    "'01-2016','02-2016','03-2016','04-2016','05-2016','06-2016','07-2016','08-2016','09-2016','10-2016','11-2016','12-2016'])\n",
    "clinton_countDF.index.name = 'username'\n",
    "clinton_countDF.reset_index(inplace=True)\n",
    "clinton_countDF = clinton_countDF.drop(columns=['username'])\n",
    "\n",
    "sentiment_scoresDF = pd.DataFrame.from_dict(sentiment_scoresDict, orient='index',columns=['01-2009','02-2009','03-2009','04-2009','05-2009','06-2009','07-2009','08-2009','09-2009','10-2009','11-2009','12-2009',\n",
    "'01-2010','02-2010','03-2010','04-2010','05-2010','06-2010','07-2010','08-2010','09-2010','10-2010','11-2010','12-2010',\n",
    "'01-2011','02-2011','03-2011','04-2011','05-2011','06-2011','07-2011','08-2011','09-2011','10-2011','11-2011','12-2011',\n",
    "'01-2012','02-2012','03-2012','04-2012','05-2012','06-2012','07-2012','08-2012','09-2012','10-2012','11-2012','12-2012',\n",
    "'01-2013','02-2013','03-2013','04-2013','05-2013','06-2013','07-2013','08-2013','09-2013','10-2013','11-2013','12-2013',\n",
    "'01-2014','02-2014','03-2014','04-2014','05-2014','06-2014','07-2014','08-2014','09-2014','10-2014','11-2014','12-2014',\n",
    "'01-2015','02-2015','03-2015','04-2015','05-2015','06-2015','07-2015','08-2015','09-2015','10-2015','11-2015','12-2015',\n",
    "'01-2016','02-2016','03-2016','04-2016','05-2016','06-2016','07-2016','08-2016','09-2016','10-2016','11-2016','12-2016'])\n",
    "sentiment_scoresDF.index.name = 'username'\n",
    "sentiment_scoresDF.reset_index(inplace=True)\n",
    "sentiment_scoresDF = sentiment_scoresDF.drop(columns=['username'])\n",
    "\n",
    "trump_sentDF = pd.DataFrame.from_dict(trump_sentDict, orient='index',columns=['01-2009','02-2009','03-2009','04-2009','05-2009','06-2009','07-2009','08-2009','09-2009','10-2009','11-2009','12-2009',\n",
    "'01-2010','02-2010','03-2010','04-2010','05-2010','06-2010','07-2010','08-2010','09-2010','10-2010','11-2010','12-2010',\n",
    "'01-2011','02-2011','03-2011','04-2011','05-2011','06-2011','07-2011','08-2011','09-2011','10-2011','11-2011','12-2011',\n",
    "'01-2012','02-2012','03-2012','04-2012','05-2012','06-2012','07-2012','08-2012','09-2012','10-2012','11-2012','12-2012',\n",
    "'01-2013','02-2013','03-2013','04-2013','05-2013','06-2013','07-2013','08-2013','09-2013','10-2013','11-2013','12-2013',\n",
    "'01-2014','02-2014','03-2014','04-2014','05-2014','06-2014','07-2014','08-2014','09-2014','10-2014','11-2014','12-2014',\n",
    "'01-2015','02-2015','03-2015','04-2015','05-2015','06-2015','07-2015','08-2015','09-2015','10-2015','11-2015','12-2015',\n",
    "'01-2016','02-2016','03-2016','04-2016','05-2016','06-2016','07-2016','08-2016','09-2016','10-2016','11-2016','12-2016'])\n",
    "trump_sentDF.index.name = 'username'\n",
    "trump_sentDF.reset_index(inplace=True)\n",
    "trump_sentDF = trump_sentDF.drop(columns=['username'])\n",
    "\n",
    "clinton_sentDF = pd.DataFrame.from_dict(clinton_sentDict, orient='index',columns=['01-2009','02-2009','03-2009','04-2009','05-2009','06-2009','07-2009','08-2009','09-2009','10-2009','11-2009','12-2009',\n",
    "'01-2010','02-2010','03-2010','04-2010','05-2010','06-2010','07-2010','08-2010','09-2010','10-2010','11-2010','12-2010',\n",
    "'01-2011','02-2011','03-2011','04-2011','05-2011','06-2011','07-2011','08-2011','09-2011','10-2011','11-2011','12-2011',\n",
    "'01-2012','02-2012','03-2012','04-2012','05-2012','06-2012','07-2012','08-2012','09-2012','10-2012','11-2012','12-2012',\n",
    "'01-2013','02-2013','03-2013','04-2013','05-2013','06-2013','07-2013','08-2013','09-2013','10-2013','11-2013','12-2013',\n",
    "'01-2014','02-2014','03-2014','04-2014','05-2014','06-2014','07-2014','08-2014','09-2014','10-2014','11-2014','12-2014',\n",
    "'01-2015','02-2015','03-2015','04-2015','05-2015','06-2015','07-2015','08-2015','09-2015','10-2015','11-2015','12-2015',\n",
    "'01-2016','02-2016','03-2016','04-2016','05-2016','06-2016','07-2016','08-2016','09-2016','10-2016','11-2016','12-2016'])\n",
    "clinton_sentDF.index.name = 'username'\n",
    "clinton_sentDF.reset_index(inplace=True)\n",
    "clinton_sentDF = clinton_sentDF.drop(columns=['username'])\n",
    "\n",
    "positive_sentDF = pd.DataFrame.from_dict(positive_sentDict, orient='index',columns=['01-2009','02-2009','03-2009','04-2009','05-2009','06-2009','07-2009','08-2009','09-2009','10-2009','11-2009','12-2009',\n",
    "'01-2010','02-2010','03-2010','04-2010','05-2010','06-2010','07-2010','08-2010','09-2010','10-2010','11-2010','12-2010',\n",
    "'01-2011','02-2011','03-2011','04-2011','05-2011','06-2011','07-2011','08-2011','09-2011','10-2011','11-2011','12-2011',\n",
    "'01-2012','02-2012','03-2012','04-2012','05-2012','06-2012','07-2012','08-2012','09-2012','10-2012','11-2012','12-2012',\n",
    "'01-2013','02-2013','03-2013','04-2013','05-2013','06-2013','07-2013','08-2013','09-2013','10-2013','11-2013','12-2013',\n",
    "'01-2014','02-2014','03-2014','04-2014','05-2014','06-2014','07-2014','08-2014','09-2014','10-2014','11-2014','12-2014',\n",
    "'01-2015','02-2015','03-2015','04-2015','05-2015','06-2015','07-2015','08-2015','09-2015','10-2015','11-2015','12-2015',\n",
    "'01-2016','02-2016','03-2016','04-2016','05-2016','06-2016','07-2016','08-2016','09-2016','10-2016','11-2016','12-2016'])\n",
    "positive_sentDF.index.name = 'username'\n",
    "positive_sentDF.reset_index(inplace=True)\n",
    "positive_sentDF = positive_sentDF.drop(columns=['username'])\n",
    "\n",
    "negative_sentDF = pd.DataFrame.from_dict(negative_sentDict, orient='index',columns=['01-2009','02-2009','03-2009','04-2009','05-2009','06-2009','07-2009','08-2009','09-2009','10-2009','11-2009','12-2009',\n",
    "'01-2010','02-2010','03-2010','04-2010','05-2010','06-2010','07-2010','08-2010','09-2010','10-2010','11-2010','12-2010',\n",
    "'01-2011','02-2011','03-2011','04-2011','05-2011','06-2011','07-2011','08-2011','09-2011','10-2011','11-2011','12-2011',\n",
    "'01-2012','02-2012','03-2012','04-2012','05-2012','06-2012','07-2012','08-2012','09-2012','10-2012','11-2012','12-2012',\n",
    "'01-2013','02-2013','03-2013','04-2013','05-2013','06-2013','07-2013','08-2013','09-2013','10-2013','11-2013','12-2013',\n",
    "'01-2014','02-2014','03-2014','04-2014','05-2014','06-2014','07-2014','08-2014','09-2014','10-2014','11-2014','12-2014',\n",
    "'01-2015','02-2015','03-2015','04-2015','05-2015','06-2015','07-2015','08-2015','09-2015','10-2015','11-2015','12-2015',\n",
    "'01-2016','02-2016','03-2016','04-2016','05-2016','06-2016','07-2016','08-2016','09-2016','10-2016','11-2016','12-2016'])\n",
    "negative_sentDF.index.name = 'username'\n",
    "negative_sentDF.reset_index(inplace=True)\n",
    "negative_sentDF = negative_sentDF.drop(columns=['username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timelineDF.to_pickle(\"./NONRESP_PAU_timelineDF_090320.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_countDF.to_pickle(\"./NONRESP_PAU_tweet_countDF_090320.pkl\")\n",
    "trump_countDF.to_pickle(\"./NONRESP_PAU_trump_countDF_090320.pkl\")\n",
    "clinton_countDF.to_pickle(\"./NONRESP_PAU_clinton_countDF_090320.pkl\")\n",
    "sentiment_scoresDF.to_pickle(\"./NONRESP_PAU_sentiment_scoresDF_090320.pkl\")\n",
    "trump_sentDF.to_pickle(\"./NONRESP_PAU_trump_sentDF_090320.pkl\")\n",
    "clinton_sentDF.to_pickle(\"./NONRESP_PAU_clinton_sentDF_090320.pkl\")\n",
    "positive_sentDF.to_pickle(\"./NONRESP_PAU_positive_sentDF_090320.pkl\")\n",
    "negative_sentDF.to_pickle(\"./NONRESP_PAU_negative_sentDF_090320.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpickled_df = pd.read_pickle(\"./negative_sentDF_090320.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "tweet_countDF.mean().plot(kind='line', color='green', figsize=(15,8),title='Mean Monthly Tweet Volume - Possibly Affected Users')\n",
    "# plt.legend(('Trump','Hillary'))\n",
    "plt.ylabel('Mean Monthly Tweet Count')\n",
    "plt.xlabel('Timeline')\n",
    "plt.savefig('PAU_graphics/NONRESP_pau_tweetcount.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "trump_countDF.mean().plot(kind='line', color='red', figsize=(15,8),title='Mean Monthly Trump/Hillary Mention Count - Possibly Affected Users')\n",
    "clinton_countDF.mean().plot(kind='line', color='blue', figsize=(15,8))\n",
    "plt.legend(('Trump','Hillary'))\n",
    "plt.ylabel('Mean Monthly Mentions')\n",
    "plt.savefig('PAU_graphics/NONRESP_pau_mentions.png')\n",
    "plt.xlabel('Timeline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sentiment_scoresDF.mean().plot(kind='line', color='green', figsize=(15,8),title='Sentiment Analysis - Possibly Affected Users')\n",
    "trump_sentDF.mean().plot(kind='line', color='red', figsize=(15,8))\n",
    "clinton_sentDF.mean().plot(kind='line', color='blue', figsize=(15,8))\n",
    "plt.legend(('General Tweets','Tweets Mentioning Trump','Tweets Mentioning Hillary'))\n",
    "plt.axhline(0, color=\"grey\", linestyle=\"--\")\n",
    "plt.ylabel('Mean Monthly Sentiment')\n",
    "plt.savefig('PAU_graphics/NONRESP_pau_sentiment.png')\n",
    "plt.xlabel('Timeline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
